# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     cell_metadata_filter: -all
#     formats: ipynb,py:percent
#     notebook_metadata_filter: jupytext,-kernelspec,-jupytext.text_representation.jupytext_version
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
# ---

# %% [markdown]
# ## ä½ç½®æ•°æ®å±•ç¤ºä¸åˆ†æç³»ç»Ÿ

# %% [markdown]
#
# ## å¼•å…¥åº“

# %%
import os
from dataclasses import dataclass
from datetime import datetime, timedelta
from io import BytesIO
from typing import Optional

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns


# %%
from geopy.distance import great_circle
from sklearn.cluster import DBSCAN

import pathmagic

with pathmagic.context():
    from func.jpfuncs import (
        add_resource_from_bytes,
        createnote,
        createresource,
        getinivaluefromcloud,
        jpapi,
        searchnotebook,
        searchnotes,
        updatenote_body,
    )
    from func.logme import log
    from func.wrapfuncs import timethis


# %% [markdown]
# ## é…ç½®å‚æ•°

# %%
@dataclass
class Config:
    """å‚æ•°é…ç½®ç±»"""

    REPORT_LEVELS: Optional[dict] = None
    PLOT_WIDTH: int = 8 # å›¾åƒå®½åº¦é»˜è®¤8è‹±å¯¸
    PLOT_HEIGHT: int = 12 # å›¾åƒé«˜åº¦é»˜è®¤12è‹±å¯¸
    DPI: int = 300 # å›¾åƒåˆ†è¾¨ç‡é»˜è®¤300
    TIME_WINDOW: str = "2h"  # åˆ¤æ–­è®¾å¤‡æ´»è·ƒçš„æ—¶é—´çª—å£ï¼Œé»˜è®¤2hï¼Œå¯ä»¥ä¸º30minç­‰æ•°å€¼
    STAY_DIST_THRESH: int = 200  # åœç•™ç‚¹è·ç¦»é˜ˆå€¼ï¼ˆç±³ï¼‰ï¼Œé»˜è®¤200ç±³
    SAMPLE_FOR_IMPORTANT_POINTS: int = 10000  # é‡è¦åœ°ç‚¹é‡‡æ ·æ•°ï¼Œé»˜è®¤10000
    RADIUS_KM: float = 1.5  # è¯†åˆ«é‡è¦åœ°ç‚¹æ—¶çš„åŠå¾„ï¼Œå•ä½ä¸ºå…¬é‡Œ
    MIN_POINTS: int = 100  # æœ€å°ç‚¹æ•°
    TIME_JUMP_DAY_THRESH: int = 30  # æ—¶é—´è·³è·ƒï¼Œç™½å¤©é˜ˆå€¼ï¼ˆåˆ†é’Ÿï¼‰
    TIME_JUMP_NIGHT_THRESH: int = 240  # æ—¶é—´è·³è·ƒï¼Œå¤œé—´é˜ˆå€¼ï¼ˆåˆ†é’Ÿï¼‰

    def __post_init__(self) -> None:
        """ä»é…ç½®è¯»å–é˜ˆå€¼ï¼Œå¦‚æœè¯»å–ä¸åˆ°åˆ™ä½¿ç”¨é»˜è®¤å€¼"""
        self.TIME_WINDOW = getinivaluefromcloud("foots", "time_window") or self.TIME_WINDOW
        self.STAY_DIST_THRESH = getinivaluefromcloud("foots", "stay_dist_thresh") or self.STAY_DIST_THRESH
        self.SAMPLE_FOR_IMPORTANT_POINTS = getinivaluefromcloud("foots", "sample_for_important_points") or self.SAMPLE_FOR_IMPORTANT_POINTS
        self.RADIUS_KM = getinivaluefromcloud("foots", "radius_km") or self.RADIUS_KM
        self.MIN_POINTS = getinivaluefromcloud("foots", "min_points") or self.MIN_POINTS
        self.TIME_JUMP_DAY_THRESH = int(getinivaluefromcloud("foots", "time_jump_day_thresh") or self.TIME_JUMP_DAY_THRESH)
        self.TIME_JUMP_NIGHT_THRESH = int(getinivaluefromcloud("foots", "time_jump_night_thresh") or self.TIME_JUMP_NIGHT_THRESH)

        if self.REPORT_LEVELS is None:
            self.REPORT_LEVELS = {
                "monthly": 1,
                "quarterly": 3,
                "yearly": 12,
                "two_year": 24,
            }


# %% [markdown]
# ## æ•°æ®åŠ è½½å‡½æ•°

# %% [markdown]
# ### load_location_data(scope, config: Config)
# åŠ è½½æŒ‡å®šèŒƒå›´çš„ä½ç½®æ•°æ®


# %%
def load_location_data(scope: str, config: Config) -> pd.DataFrame:
    """åŠ è½½æŒ‡å®šèŒƒå›´çš„ä½ç½®æ•°æ®"""
    # è·å–åŒ…å«å½“å‰æœˆä»½ç¬¬ä¸€å¤©æ—¥æœŸçš„åˆ—è¡¨
    end_date = datetime.now()
    months = config.REPORT_LEVELS[scope]
    start_date = end_date - timedelta(days=30 * months)
    date_range = pd.date_range(start_date, end_date, freq="MS")
    monthly_dfs = []

    for date in date_range:
        month_str = date.strftime("%Y%m")
        note_title = f"ä½ç½®æ•°æ®_{month_str}"
        notes = searchnotes(f"{note_title}")

        if not notes:
            log.warning(f"æœªæ‰¾åˆ°{month_str}çš„ä½ç½®æ•°æ®ç¬”è®°")
            continue

        note = notes[0]
        resources = jpapi.get_resources(note.id).items

        location_resource = None
        for res in resources:
            if res.title.endswith(".xlsx"):
                location_resource = res
                break

        if not location_resource:
            log.warning(f"æœªæ‰¾åˆ°{month_str}çš„ä½ç½®æ•°æ®é™„ä»¶")
            continue

        res_data = jpapi.get_resource_file(location_resource.id)
        df = pd.read_excel(BytesIO(res_data))
        df["month"] = month_str
        monthly_dfs.append(df)

    if not monthly_dfs:
        log.warning(f"æœªæ‰¾åˆ°{scope}çš„ä½ç½®æ•°æ®")
        return pd.DataFrame()

    return pd.concat(monthly_dfs).reset_index(drop=True)

# %% [markdown]
# ## æ•°æ®åˆ†æå‡½æ•°

# %% [markdown]
# ### analyze_location_data(df, scope)


# %%
@timethis
def analyze_location_data(indf: pd.DataFrame, scope: str) -> dict:
    """åˆ†æä½ç½®æ•°æ®ï¼Œè¿”å›ç»Ÿè®¡ç»“æœ

    ä¿®å¤åˆ—åé—®é¢˜å¹¶æ·»åŠ æ•°æ®é¢„å¤„ç†
    """
    config = Config()
    df = indf.copy()

    # 1. æ•°æ®é¢„å¤„ç†
    # 1.1. æŒ‰è®¾å¤‡å’Œæ—¶é—´åˆ—å»é‡
    df = df.sort_values(by=["device_id", "time"]).drop_duplicates(subset=["device_id", "time"])
    print(
        f"å»é‡åå¤§å°ä¸ºï¼š{df.shape[0]}ï¼›èµ·è‡ª{df['time'].min()}ï¼Œæ­¢äº{df['time'].max()}ã€‚"
    )
    print(df.groupby("device_id").count()["time"])

    # 1.2 è®¾å¤‡èåˆ
    print(
        f"èåˆè®¾å¤‡æ•°æ®å‰å¤§å°ä¸ºï¼š{df.shape[0]}ï¼›èµ·è‡ª{df['time'].min()}ï¼Œæ­¢äº{df['time'].max()}ã€‚"
    )
    print(df.groupby("device_id").count()["time"])
    df = fuse_device_data(df, config)
    # df = fuse_device_data_dask(df, config)
    print(
        f"èåˆè®¾å¤‡æ•°æ®åå¤§å°ä¸ºï¼š{df.shape[0]}ï¼›èµ·è‡ª{df['time'].min()}ï¼Œæ­¢äº{df['time'].max()}ã€‚"
    )
    print(df.groupby("device_id").count()["time"])

    # 1.3. å¤„ç†æ—¶é—´è·³è·ƒï¼Œæ·»åŠ time_diffåˆ—ï¼Œbig_gapåˆ—å’Œsegmentåˆ—
    df = handle_time_jumps(df, config)

    # 1.4. ä½ç½®å¹³æ»‘
    df = smooth_coordinates(df)
    print(
        f"å¤„ç†èåˆè®¾å¤‡ã€æ—¶é—´è·³è·ƒå’Œä½ç½®å¹³æ»‘åè®¾å¤‡æ•°æ®åå¤§å°ä¸ºï¼š{df.shape[0]}ï¼›èµ·è‡ª{df['time'].min()}ï¼Œæ­¢äº{df['time'].max()}ã€‚"
    )
    print(df.groupby("device_id").count()["time"])

    # 1.5. é‡è¦åœ°ç‚¹åˆ†æ
    clustered = identify_important_places(df, config)
    if "cluster" in clustered.columns:
        df["cluster"] = clustered["cluster"]
    print(f"é‡è¦åœ°ç‚¹åˆ†æåæ•°æ®åˆ—ä¸º: {df.columns.tolist()}")

    # 2. è®¡ç®—åˆ†æç»“æœ

    # 2.1 æ—¶é—´èŒƒå›´åˆ†æ
    start_time = df["time"].min().strftime("%Y-%m-%d")
    end_time = df["time"].max().strftime("%Y-%m-%d")

    # 2.2 åŸºæœ¬ç»Ÿè®¡
    total_points = len(df)
    unique_days = df["time"].dt.date.nunique()

    # 2.3 è®¾å¤‡åˆ†æ
    device_stats = df["device_id"].value_counts().to_dict()

    # 2.4 è·ç¦»
    min_lat, max_lat = df["latitude"].min(), df["latitude"].max()
    min_lon, max_lon = df["longitude"].min(), df["longitude"].max()
    distance_km = great_circle((min_lat, min_lon), (max_lat, max_lon)).kilometers

    # 2.5 å¤§è·¨è¶Š
    if "big_gap" in df.columns:
        big_gaps = df[df["big_gap"]]
        gap_stats = {
            "count": len(big_gaps),
            "longest_gap": df["time_diff"].max() if "time_diff" in df.columns else 0,
        }
    else:
        gap_stats = {"count": 0, "longest_gap": 0}

    # 2.6 å°æ—¶åˆ†å¸ƒ
    df["hour"] = df["time"].dt.hour
    hourly_distribution = df["hour"].value_counts().sort_index().to_dict()

    # 2.7 ç²¾åº¦åˆ†æ
    accuracy_stats = {
        "min": df["accuracy"].min(),
        "max": df["accuracy"].max(),
        "mean": df["accuracy"].mean(),
    }

    # 2.8 é‡è¦åœ°ç‚¹åˆ†æ
    if "cluster" in df.columns:
        important_places = (
            df[df["cluster"] >= 0]
            .groupby("cluster")
            .agg(
                {
                    "latitude": "mean",
                    "longitude": "mean",
                }
            )
            .assign(visit_count=1)
            .assign(avg_stay_min=0)
            .sort_values("visit_count", ascending=False)
            .head(5)
        )
    else:
        important_places = pd.DataFrame()

    # 2.9 åœç•™ç‚¹åˆ†æ
    df = identify_stay_points(df, config)
    # è®¡ç®—åœç•™ç‚¹ç»Ÿè®¡
    stay_stats = {
        "total_stays": df["stay_group"].nunique(),  # è°ƒæ•´total_staysçš„è®¡ç®—é€»è¾‘
        "avg_duration": df[df["is_stay"]]["duration"].mean() / 60
        if "duration" in df
        else 0,
        "top_locations": df[df["is_stay"]]
        .groupby("cluster")
        .size()
        .nlargest(3)
        .to_dict(),
    }
    stay_stats["resource_id"] = generate_stay_points_map(df, scope, config)
    print(f"åˆ†æå®Œæˆåæ•°æ®åˆ—ä¸º: {df.columns.tolist()}")

    # éšæœºé€‰æ‹©ä¸€ä¸ªstay_groupå€¼ï¼Œå¹¶æ‰“å°è¯¥å€¼ç¬¬ä¸€æ¬¡å‡ºç°çš„å‰äº”æ¡è®°å½•å’Œåäº”æ¡è®°å½•
    if df["stay_group"].isna().all():
        print("æ²¡æœ‰æ‰¾åˆ°stay_groupçš„è®°å½•ã€‚")
    else:
        stay_groups = df[df["stay_group"].notna()]["stay_group"].unique()
        random_stay_group = np.random.choice(stay_groups)
        first_occurrence_index = df[df["stay_group"] == random_stay_group].index[0]
        print(f"éšæœºé€‰å–çš„stay_groupä¸º: {random_stay_group}")
        print("è¯¥stay_groupç¬¬ä¸€æ¬¡å‡ºç°çš„å‰äº”æ¡è®°å½•å’Œåäº”æ¡è®°å½•å¦‚ä¸‹ï¼š")
        print(df.iloc[max(0, first_occurrence_index-5):first_occurrence_index+6])

    # 3. ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–èµ„æº
    analysis_results = {
        "scope": scope,
        "time_range": (start_time, end_time),
        "total_points": total_points,
        "unique_days": unique_days,
        "distance_km": distance_km,
        "device_stats": device_stats,
        "hourly_distribution": hourly_distribution,
        "gap_stats": gap_stats,
        "accuracy_stats": accuracy_stats,
        "important_places": important_places.to_dict("records"),
        "stay_stats": stay_stats,
    }
    resource_ids = {}
    # 3.1 è½¨è¿¹å›¾
    resource_ids["trajectory_with_map"] = generate_trajectory_map(df, scope, config)

    # ä¿ç•™åŸå§‹è½¨è¿¹å›¾ä½œä¸ºå¤‡é€‰
    # resource_ids["trajectory"] = generate_trajectory_map_fallback(df, scope, config)

    # 3.2 è®¾å¤‡åˆ†å¸ƒé¥¼å›¾
    resource_ids["device_dist"] = generate_device_pie_chart(
        analysis_results["device_stats"]
    )

    # 3.3 æ—¶é—´åˆ†å¸ƒçƒ­åŠ›å›¾
    resource_ids["time_heatmap"] = generate_time_heatmap(
        analysis_results["hourly_distribution"]
    )

    # 3.4 ç²¾åº¦åˆ†å¸ƒå›¾
    if "accuracy" in df.columns:
        plt.figure(figsize=(config.PLOT_WIDTH, config.PLOT_HEIGHT - 2))
        sns.histplot(df["accuracy"].dropna(), bins=30, kde=True, color="#55a868")
        plt.title(f"{scope.capitalize()}å®šä½ç²¾åº¦åˆ†å¸ƒ")
        plt.xlabel("ç²¾åº¦ (ç±³)")
        plt.grid(True, alpha=0.3)
        buf_acc = BytesIO()
        plt.savefig(buf_acc, format="png", dpi=config.DPI)
        plt.close()
        resource_ids["accuracy"] = add_resource_from_bytes(
            buf_acc.getvalue(), f"ç²¾åº¦åˆ†å¸ƒ_{scope}.png"
        )

    # 3.5 åœç•™ç‚¹åœ°å›¾ï¼ˆå·²åœ¨å‰é¢è®¡ç®—ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨ï¼‰
    resource_ids["stay_points_map"] = analysis_results["stay_stats"]["resource_id"]

    # 3.6 äº¤äº’å¼åœ°å›¾
    resource_ids["interactive_map"] = generate_interactive_map(df, scope, config)

    # 3.7 æ—¶é—´åºåˆ—åˆ†æ
    resource_ids["time_series"] = generate_time_series_analysis(df, scope, config)

    # 3.8 æ·±åº¦åœç•™åˆ†æ
    resource_ids["enhanced_stays"] = enhanced_stay_points_analysis(df, scope, config)

    # 3.9 ç§»åŠ¨æ¨¡å¼è¯†åˆ«
    resource_ids["movement_patterns"] = movement_pattern_analysis(df, scope, config)

    # 3.10 æ•°æ®è´¨é‡ç›‘æ§
    resource_ids["data_quality"] = data_quality_dashboard(df, scope, config)

    # å°†èµ„æº ID æ·»åŠ åˆ°åˆ†æç»“æœä¸­
    analysis_results["resource_ids"] = resource_ids

    return analysis_results


# %% [markdown]
# ### fuse_device_data(df, config: Config)


# %%
def fuse_device_data(df: pd.DataFrame, config: Config) -> pd.DataFrame:
    """å¤šè®¾å¤‡æ•°æ®æ™ºèƒ½é€‰æ‹©ï¼šæ¯ä¸ªæ—¶é—´çª—å£é€‰æ‹©æœ€ä½³è®¾å¤‡çš„æ•°æ®"""
    print(f"å¤šè®¾å¤‡æ•°æ®æ™ºèƒ½é€‰æ‹©æ—¶é—´çª—å£ä¸ºï¼š{config.TIME_WINDOW}")

    # 1. åˆ›å»ºæ—¶é—´çª—å£
    df["time_window"] = df["time"].dt.floor(config.TIME_WINDOW)

    # 2. å­˜å‚¨æœ€ç»ˆé€‰æ‹©çš„æ•°æ®ç‚¹
    selected_points = []

    for window, group in df.groupby("time_window"):
        if len(group) == 0:
            continue

        # 3. è®¡ç®—æ¯ä¸ªè®¾å¤‡çš„ç»¼åˆè¯„åˆ†
        device_scores = {}
        for device_id, device_data in group.groupby("device_id"):
            # 3.1 è®¡ç®—è®¾å¤‡æ´»è·ƒåº¦
            activity = calc_device_activity_optimized(device_data, device_id)

            # 3.2 è®¡ç®—è®¾å¤‡å¹³å‡ç²¾åº¦ï¼ˆç²¾åº¦è¶Šé«˜è¶Šå¥½ï¼‰
            avg_accuracy = device_data["accuracy"].mean()

            # 3.3 è®¡ç®—è®¾å¤‡ä½ç½®ç¨³å®šæ€§
            lat_std = device_data["latitude"].std()
            lon_std = device_data["longitude"].std()
            stability = 1 / (lat_std + lon_std + 1e-6)  # é¿å…é™¤é›¶

            # 3.4 ç»¼åˆè¯„åˆ† = æ´»è·ƒåº¦ * ç¨³å®šæ€§ * (1/å¹³å‡ç²¾åº¦)
            score = activity * stability * (1 / max(avg_accuracy, 1))
            device_scores[device_id] = score

        # 4. é€‰æ‹©è¯„åˆ†æœ€é«˜çš„è®¾å¤‡
        best_device = max(device_scores, key=device_scores.get)

        # 5. è·å–è¯¥è®¾å¤‡åœ¨æœ¬æ—¶é—´çª—å£çš„æ‰€æœ‰æ•°æ®ç‚¹
        best_device_data = group[group["device_id"] == best_device]

        # 6. æ·»åŠ å…ƒæ•°æ®
        best_device_data = best_device_data.copy()
        best_device_data["selected_device"] = best_device
        best_device_data["selection_score"] = device_scores[best_device]

        selected_points.append(best_device_data)

    # 7. åˆå¹¶æ‰€æœ‰é€‰æ‹©çš„æ•°æ®ç‚¹
    result_df = pd.concat(selected_points)
    return result_df

# %% [markdown]
# ### calc_device_activity(df, device_id)


# %%
def calc_device_activity(df: pd.DataFrame, device_id: str) -> int:
    """è®¡ç®—è®¾å¤‡æ´»è·ƒåº¦è¯„åˆ†ï¼ˆ0-100ï¼‰"""
    device_data = df[df["device_id"] == device_id].copy()
    if len(device_data) < 2:
        return 0

    total_dist = 0
    prev = None
    for _, row in device_data.iterrows():
        if prev is not None:
            dist = great_circle(
                (prev.latitude, prev.longitude), (row.latitude, row.longitude)
            ).m
            total_dist += dist
        prev = row

    time_span = (
        device_data["time"].max() - device_data["time"].min()
    ).total_seconds() / 3600
    lat_var = device_data["latitude"].var()
    lon_var = device_data["longitude"].var()

    activity_score = min(
        100,
        int((total_dist / max(1, time_span)) * 0.7 + (lat_var + lon_var) * 10000 * 0.3),
    )
    return activity_score


# %% [markdown]
# ### calc_device_activity_optimized(df, device_id)

# %%
def calc_device_activity_optimized(df: pd.DataFrame, device_id: str) -> int:
    """ä¼˜åŒ–ç‰ˆè®¾å¤‡æ´»è·ƒåº¦è¯„åˆ†"""
    device_data = df[df["device_id"] == device_id].copy()

    # åŸºç¡€æ ¡éªŒ
    if len(device_data) < 2:
        return 0

    # å‘é‡åŒ–è·ç¦»è®¡ç®—
    coords = device_data[["latitude", "longitude"]].values
    dists = np.zeros(len(coords) - 1)
    for i in range(1, len(coords)):
        dists[i - 1] = great_circle(coords[i - 1], coords[i]).m
    total_dist = np.sum(dists)

    # æ—¶é—´è·¨åº¦è®¡ç®—
    time_min = device_data["time"].min()
    time_max = device_data["time"].max()
    time_span = max(0.1, (time_max - time_min).total_seconds() / 3600)

    # ä½ç½®å˜åŒ–è®¡ç®—
    lat_deg_to_m = 111000
    mean_lat = np.radians(device_data["latitude"].mean())
    lon_deg_to_m = 111000 * np.cos(mean_lat)

    lat_std_m = device_data["latitude"].std() * lat_deg_to_m
    lon_std_m = device_data["longitude"].std() * lon_deg_to_m
    pos_variation = np.sqrt(lat_std_m**2 + lon_std_m**2)

    # æ”¹è¿›è¯„åˆ†å…¬å¼
    distance_score = min(100, total_dist / time_span) * 0.7
    variation_score = min(100, pos_variation / 1000) * 0.3

    return min(100, int(distance_score + variation_score))


# %% [markdown]
# ### smooth_coordinates(df, window_size=5)

# %%
def smooth_coordinates(df: pd.DataFrame, window_size: int=5) -> pd.DataFrame:
    """ä½¿ç”¨æ»‘åŠ¨çª—å£å¹³å‡æ³•å¹³æ»‘ç»çº¬åº¦åæ ‡

    å‚æ•°:
        window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆå¥‡æ•°ï¼‰
    """
    # ç¡®ä¿æŒ‰æ—¶é—´æ’åº
    df = df.sort_values("time")

    # ä½¿ç”¨æ»šåŠ¨çª—å£è®¡ç®—å¹³å‡ä½ç½®
    df["smoothed_lat"] = (
        df["latitude"].rolling(window=window_size, center=True, min_periods=1).mean()
    )

    df["smoothed_lon"] = (
        df["longitude"].rolling(window=window_size, center=True, min_periods=1).mean()
    )

    # å¯¹äºè¾¹ç¼˜ç‚¹ï¼Œä½¿ç”¨åŸå§‹å€¼
    df["smoothed_lat"] = df["smoothed_lat"].fillna(df["latitude"])
    df["smoothed_lon"] = df["smoothed_lon"].fillna(df["longitude"])

    return df

# %% [markdown]
# ### handle_time_jumps(df, config: Config)


# %%
def handle_time_jumps(df: pd.DataFrame, config: Config) -> pd.DataFrame:
    """æ™ºèƒ½å¤„ç†æ—¶é—´è·³è·ƒï¼Œè€ƒè™‘ä½ç½®å˜åŒ–å’Œè®¾å¤‡åˆ‡æ¢

    é¿å…è¿‡åº¦åˆ†å‰²è¿ç»­è½¨è¿¹
    """
    if df.empty:
        return df

    # 1. æ’åºå¹¶è®¡ç®—æ—¶é—´å·®
    df = df.sort_values("time")
    df["time_diff"] = df["time"].diff().dt.total_seconds().fillna(0) / 60

    # 2. åŠ¨æ€é˜ˆå€¼è®¡ç®—ï¼ˆåŸºäºæ´»åŠ¨æ¨¡å¼ï¼‰
    # å·¥ä½œæ—¥ç™½å¤©é˜ˆå€¼è¾ƒä½ï¼ˆ30åˆ†é’Ÿï¼‰ï¼Œå¤œé—´é˜ˆå€¼è¾ƒé«˜ï¼ˆ4å°æ—¶ï¼‰
    hour = df["time"].dt.hour
    is_weekday = df["time"].dt.dayofweek < 6
    day_threshold = config.TIME_JUMP_DAY_THRESH  # 30åˆ†é’Ÿ
    night_threshold = config.TIME_JUMP_NIGHT_THRESH  # 4å°æ—¶

    # åŠ¨æ€é˜ˆå€¼ï¼šç™½å¤©å·¥ä½œæ—¶é—´é˜ˆå€¼ä½ï¼Œå¤œé—´é˜ˆå€¼é«˜
    df["dynamic_threshold"] = np.where(
        (hour >= 8) & (hour <= 20) & is_weekday, day_threshold, night_threshold
    )

    # 3. æ™ºèƒ½è·³è·ƒæ£€æµ‹ï¼ˆç»“åˆæ—¶é—´å’Œä½ç½®å˜åŒ–ï¼‰
    df["prev_lat"] = df["latitude"].shift(1)
    df["prev_lon"] = df["longitude"].shift(1)

    # è®¡ç®—ä½ç½®å˜åŒ–ï¼ˆç±³ï¼‰
    df["dist_change"] = df.apply(
        lambda row: great_circle(
            (row["prev_lat"], row["prev_lon"]), (row["latitude"], row["longitude"])
        ).m
        if not pd.isna(row["prev_lat"])
        else 0,
        axis=1,
    )

    # 4. è·³è·ƒæ¡ä»¶ï¼šæ—¶é—´å·®è¶…è¿‡é˜ˆå€¼ä¸”ä½ç½®å˜åŒ–å°ï¼ˆå¯èƒ½ä¸ºè®¾å¤‡åˆ‡æ¢æˆ–é™æ­¢ï¼‰
    df["big_gap"] = (df["time_diff"] > df["dynamic_threshold"]) & (
        df["dist_change"] < config.STAY_DIST_THRESH
    )

    # 5. è®¾å¤‡åˆ‡æ¢æ£€æµ‹ï¼ˆé¢å¤–æ ‡è®°ï¼‰
    df["device_change"] = df["device_id"] != df["device_id"].shift(1)

    # 6. æ™ºèƒ½åˆ†æ®µé€»è¾‘
    # ç»„åˆæ—¶é—´è·³è·ƒå’Œè®¾å¤‡åˆ‡æ¢ä½œä¸ºåˆ†æ®µç‚¹
    df["segment_point"] = df["big_gap"] | df["device_change"]
    df["segment"] = df["segment_point"].cumsum()

    # æ¸…ç†ä¸´æ—¶åˆ—
    df.drop(
        ["prev_lat", "prev_lon", "dynamic_threshold", "segment_point", "device_change"],
        axis=1,
        inplace=True,
        errors="ignore",
    )

    return df


# %% [markdown]
# ### check_spatiotemporal_consistency(point1, point2)


# %%
def check_spatiotemporal_consistency(point1: pd.Series, point2: pd.Series) -> bool:
    """æ£€æŸ¥ä¸¤ç‚¹æ—¶ç©ºä¸€è‡´æ€§"""
    time_diff = abs((point1["time"] - point2["time"]).total_seconds())
    dist = great_circle(
        (point1.latitude, point1.longitude), (point2.latitude, point2.longitude)
    ).m
    max_allowed_dist = min(100, time_diff * 0.5)  # 0.5m/sç§»åŠ¨é€Ÿåº¦
    return dist < max_allowed_dist and time_diff < 300


# %% [markdown]
# ### detect_static_devices(df, var_threshold=0.0002)


# %%
def detect_static_devices(df: pd.DataFrame, var_threshold: float=0.0002) -> pd.DataFrame:
    """è¯†åˆ«å¹¶è¿‡æ»¤é™æ€è®¾å¤‡"""
    static_devices = []
    for device_id, device_data in df.groupby("device_id"):
        lat_var = device_data["latitude"].var()
        lon_var = device_data["longitude"].var()

        if lat_var < var_threshold and lon_var < var_threshold:
            static_devices.append(device_id)
            log.info(f"è®¾å¤‡ {device_id} è¢«è¯†åˆ«ä¸ºé™æ€è®¾å¤‡")

    return df[~df["device_id"].isin(static_devices)]


# %% [markdown]
# ### identify_stay_points(df, config)

# %%
def identify_stay_points(df: pd.DataFrame, config: Config) -> pd.DataFrame:
    """è¯†åˆ«åœç•™ç‚¹å¹¶åšç›¸åº”å¤„ç†ï¼Œå¢åŠ æ•°æ®åˆ—is_stayã€stay_groupã€duration"""
    # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åº
    df = df.sort_values("time").reset_index(drop=True)

    # æ·»åŠ å‰ä¸€ä½ç½®åˆ—
    df["prev_lat"] = df["smoothed_lat"].shift(1)
    df["prev_lon"] = df["smoothed_lon"].shift(1)

    # è®¡ç®—è·ç¦»
    df["dist_to_prev"] = df.apply(
        lambda row: great_circle(
            (row["smoothed_lat"], row["smoothed_lon"]),
            (row["prev_lat"], row["prev_lon"]),
        ).meters
        if not pd.isna(row["prev_lat"])
        else 0,
        axis=1,
    )

    # åˆå§‹åŒ–is_stayåˆ—ä¸ºFalse
    df["is_stay"] = False

    # åˆå§‹åŒ–stay_groupåˆ—
    df["stay_group"] = None

    # åˆå§‹åŒ–durationåˆ—
    df["duration"] = None

    # æ ‡è®°åœç•™ç‚¹
    stay_group_counter = 0
    current_stay_group = None

    for i in range(1, len(df)):
        if (df.loc[i, "dist_to_prev"] < config.STAY_DIST_THRESH):
            if current_stay_group is None:
                stay_group_counter += 1
                current_stay_group = stay_group_counter
            df.loc[i, "is_stay"] = True
            df.loc[i, "stay_group"] = current_stay_group
        else:
            current_stay_group = None

    # è®¡ç®—æ¯ç»„åœç•™æ—¶é—´
    stay_groups = df[df["is_stay"]].groupby("stay_group")
    df.loc[df["is_stay"], "duration"] = stay_groups["time_diff"].transform("sum")

    # åˆ é™¤è¿‡ç¨‹æ•°æ®åˆ—prev_latå’Œprev_lon
    df.drop(columns=["prev_lat", "prev_lon"], inplace=True)

    return df



# %% [markdown]
# ### identify_important_places(df, radius_km=1.5, min_points=200)
# è¯†åˆ«é‡è¦åœ°ç‚¹ï¼ˆåœç•™ç‚¹ï¼‰


# %%
@timethis
def identify_important_places(df: pd.DataFrame, config: Config, radius_km: float=1.5, min_points: int=200) -> pd.DataFrame:
    """è¯†åˆ«é‡è¦åœ°ç‚¹

    1.5å…¬é‡ŒåŠå¾„å†…çš„ç‚¹æ•°é‡å¤§äº200ä¸ªï¼Œåˆ™è®¤ä¸ºæ˜¯é‡è¦åœ°ç‚¹ã€‚

    Args:
        df (pd.DataFrame): åŸå§‹æ•°æ®
        config (Config): é…ç½®ä¿¡æ¯
        radius_km (float, optional): åŠå¾„ï¼Œå•ä½ä¸ºå…¬é‡Œ. Defaults to 1.5.
        min_points (int, optional): æœ€å°ç‚¹æ•°. Defaults to 200.

    Returns:
        pd.DataFrame: é‡è¦åœ°ç‚¹æ•°æ®
    """
    # log.info(f"è¯†åˆ«é‡è¦åœ°ç‚¹åˆå§‹æ•°æ®è®°å½•æ•°ä¸ºï¼š\t{df.shape[0]}")
    # ä½¿ç”¨å¹³æ»‘åçš„åæ ‡
    if "smoothed_lat" in df.columns and "smoothed_lon" in df.columns:
        coords = df[["smoothed_lat", "smoothed_lon"]].values
    else:
        coords = df[["latitude", "longitude"]].values

    # ä¼˜åŒ–1ï¼šå¯¹æ•°æ®è¿›è¡Œé‡‡æ ·ï¼Œå‡å°‘å¤„ç†é‡
    sample_size = min(config.SAMPLE_FOR_IMPORTANT_POINTS, len(coords))
    if len(coords) > sample_size:
        indices = np.random.choice(len(coords), sample_size, replace=False)
        coords = coords[indices]
    # log.info(f"è¯†åˆ«é‡è¦åœ°ç‚¹åˆå§‹æ•°æ®è®°å½•æ•°æŠ½æ ·åä¸ºï¼š\t{len(coords)}")

    kms_per_radian = 6371.0088
    epsilon = radius_km / kms_per_radian  # 1500ç±³åŠå¾„

    # ä¼˜åŒ–3ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„ç®—æ³•å‚æ•°
    db = DBSCAN(
        eps=epsilon,
        min_samples=min_points,
        algorithm="ball_tree",
        metric="haversine",
        n_jobs=-1,  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒå¹¶è¡Œè®¡ç®—
    ).fit(np.radians(coords))

    # ä¸ºåŸå§‹æ•°æ®æ·»åŠ èšç±»æ ‡ç­¾
    df["cluster"] = -1  # é»˜è®¤-1è¡¨ç¤ºå™ªå£°ç‚¹
    if len(coords) < len(df):
        # åªæ›´æ–°é‡‡æ ·ç‚¹çš„èšç±»æ ‡ç­¾
        df.iloc[indices, df.columns.get_loc("cluster")] = db.labels_
    else:
        df["cluster"] = db.labels_

    # åªä¿ç•™æœ‰æ•ˆèšç±»ï¼ˆæ’é™¤å™ªå£°ç‚¹ï¼‰
    clustered = df[df["cluster"] >= 0]

    return clustered


# %% [markdown]
# ### identify_important_places_before(df, radius_km=0.5, min_points=3)

# %%
def identify_important_places_before(df: pd.DataFrame, radius_km: float=0.5, min_points: int=3) -> pd.DataFrame:
    """è¯†åˆ«é‡è¦åœ°ç‚¹ï¼ˆåœç•™ç‚¹ï¼‰

    å‡å°èšç±»åŠå¾„ä»¥å¤„ç†ä½ç½®æ‰°åŠ¨
    """
    # ä½¿ç”¨å¹³æ»‘åçš„åæ ‡
    if "smoothed_lat" in df.columns and "smoothed_lon" in df.columns:
        coords = df[["smoothed_lat", "smoothed_lon"]].values
    else:
        coords = df[["latitude", "longitude"]].values

    # å°†åŠå¾„ä»ç±³è½¬æ¢ä¸ºåº¦ï¼ˆè¿‘ä¼¼ï¼‰
    kms_per_radian = 6371.0088
    epsilon = radius_km / kms_per_radian

    # ä½¿ç”¨DBSCANèšç±»
    db = DBSCAN(
        eps=epsilon, min_samples=min_points, algorithm="ball_tree", metric="haversine"
    ).fit(np.radians(coords))

    df["cluster"] = db.labels_

    # åªä¿ç•™æœ‰æ•ˆèšç±»ï¼ˆæ’é™¤å™ªå£°ç‚¹ï¼‰
    clustered = df[df["cluster"] >= 0]

    return clustered


# %% [markdown]
# ## å¯è§†åŒ–å‡½æ•°

# %% [markdown]
# ### generate_device_pie_chart(device_stats)

# %%
def generate_device_pie_chart(device_stats: dict) -> str:
    """ç”Ÿæˆè®¾å¤‡åˆ†å¸ƒé¥¼å›¾

    return res_id: str
    """
    plt.figure(figsize=(6, 6))
    labels = [
        getinivaluefromcloud("device", str(device_id)) for device_id in device_stats
    ]
    sizes = list(device_stats.values())
    plt.pie(sizes, labels=labels, autopct="%1.1f%%", startangle=90)
    plt.axis("equal")

    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=120)
    plt.close()
    return add_resource_from_bytes(buf.getvalue(), "è®¾å¤‡åˆ†å¸ƒ.png")


# %% [markdown]
# ### generate_time_heatmap(hourly_distribution)

# %%
def generate_time_heatmap(hourly_distribution: dict) -> str:
    """ç”Ÿæˆ24å°æ—¶çƒ­åŠ›å›¾

    return res_id: str
    """
    hours = list(range(24))
    values = [hourly_distribution.get(h, 0) for h in hours]

    plt.figure(figsize=(10, 3))
    plt.bar(hours, values, color="#4c72b0")
    plt.xticks(hours)
    plt.xlabel("å°æ—¶")
    plt.ylabel("è®°å½•æ•°")
    plt.grid(axis="y", alpha=0.3)

    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=120)
    plt.close()
    return add_resource_from_bytes(buf.getvalue(), "æ—¶é—´åˆ†å¸ƒçƒ­åŠ›å›¾.png")


# %% [markdown]
# ### generate_geo_link(lat, lon)

# %%
def generate_geo_link(lat: float, lon: float) -> str:
    """ç”Ÿæˆåœ°å›¾é“¾æ¥"""
    return f" https://www.openstreetmap.org/?mlat={lat}&mlon={lon}&zoom=15"


# %% [markdown]
# ### generate_trajectory_map(df, scope, config)

# %%
def generate_trajectory_map(df: pd.DataFrame, scope: str, config: Config) -> str:
    """ç”Ÿæˆå¸¦åœ°å›¾åº•å›¾çš„è½¨è¿¹å›¾ï¼ˆä¼˜åŒ–ç‰ˆï¼‰- æ˜¾ç¤ºåˆ†æ®µèµ·å§‹æ—¥æœŸ

    scope: strï¼Œæ—¥æœŸèŒƒå›´åç§°
    """
    try:
        import contextily as ctx

        fig, ax = plt.subplots(figsize=(config.PLOT_WIDTH, config.PLOT_HEIGHT))

        # 1. ä¼˜åŒ–å›¾ä¾‹å¤„ç† - åªæ˜¾ç¤ºæœ€æ–°çš„6ä¸ªåˆ†æ®µï¼Œå¹¶æ˜¾ç¤ºèµ·å§‹æ—¥æœŸ
        max_legend_items = 6  # æœ€å¤šæ˜¾ç¤º6ä¸ªå›¾ä¾‹é¡¹

        if "segment" in df.columns:
            # è·å–æ‰€æœ‰åˆ†æ®µå¹¶æŒ‰èµ·å§‹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            segments = df["segment"].unique()

            # è®¡ç®—æ¯ä¸ªåˆ†æ®µçš„èµ·å§‹æ—¶é—´
            segment_start_time = {}
            for segment in segments:
                seg_df = df[df["segment"] == segment]
                segment_start_time[segment] = seg_df[
                    "time"
                ].min()  # ä½¿ç”¨min()è·å–èµ·å§‹æ—¶é—´

            # æŒ‰èµ·å§‹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            sorted_segments = sorted(
                segments, key=lambda x: segment_start_time[x], reverse=True
            )

            # åªä¿ç•™å‰6ä¸ªæœ€æ–°åˆ†æ®µ
            segments_to_show = sorted_segments[:max_legend_items]

            # ç»˜åˆ¶æ‰€æœ‰åˆ†æ®µä½†åªæ˜¾ç¤ºæœ€æ–°6ä¸ªçš„å›¾ä¾‹
            for segment in segments:
                seg_df = df[df["segment"] == segment]
                if segment in segments_to_show:
                    # æ ¼å¼åŒ–æ—¥æœŸä¸º"25å¹´9æœˆ1æ—¥"çš„æ ¼å¼
                    start_date_str = segment_start_time[segment].strftime(
                        "%yå¹´%-mæœˆ%-dæ—¥"
                    )

                    ax.plot(
                        seg_df["longitude"],
                        seg_df["latitude"],
                        alpha=0.7,
                        linewidth=2.0,
                        label=f"{start_date_str}",
                    )
                else:
                    ax.plot(
                        seg_df["longitude"],
                        seg_df["latitude"],
                        alpha=0.7,
                        linewidth=2.0,
                        color="gray",  # ä½¿ç”¨ç°è‰²è¡¨ç¤ºä¸æ˜¾ç¤ºå›¾ä¾‹çš„åˆ†æ®µ
                    )
        else:
            # æ²¡æœ‰åˆ†æ®µæ•°æ®
            ax.plot(df["longitude"], df["latitude"], "b-", alpha=0.7, linewidth=2.0)

        # 2. ä¼˜åŒ–è¾¹ç•Œè®¡ç®—
        min_lon, max_lon = df["longitude"].min(), df["longitude"].max()
        min_lat, max_lat = df["latitude"].min(), df["latitude"].max()

        lon_range = max_lon - min_lon
        lat_range = max_lat - min_lat

        # åŠ¨æ€è®¡ç®—è¾¹è· - åŸºäºæ•°æ®èŒƒå›´çš„æ¯”ä¾‹
        if lon_range < 0.1 or lat_range < 0.1:  # å°èŒƒå›´æ•°æ®
            margin_factor = 0.15  # 15%çš„è¾¹è·
        else:  # å¤§èŒƒå›´æ•°æ®
            margin_factor = 0.05  # 5%çš„è¾¹è·

        lon_margin = lon_range * margin_factor
        lat_margin = lat_range * margin_factor

        # ç¡®ä¿æœ€å°è¾¹è·ï¼ˆé¿å…æ•°æ®ç‚¹å¤ªé è¿‘è¾¹ç¼˜ï¼‰
        min_abs_margin = 0.005  # æœ€å°ç»å¯¹è¾¹è·ï¼ˆåº¦ï¼‰
        lon_margin = max(lon_margin, min_abs_margin)
        lat_margin = max(lat_margin, min_abs_margin)

        ax.set_xlim(min_lon - lon_margin, max_lon + lon_margin)
        ax.set_ylim(min_lat - lat_margin, max_lat + lat_margin)

        # 3. è®¡ç®—åˆé€‚çš„ç¼©æ”¾çº§åˆ«
        lon_range = max_lon - min_lon
        lat_range = max_lat - min_lat
        max_range = max(lon_range, lat_range)

        # æ ¹æ®æ•°æ®èŒƒå›´åŠ¨æ€ç¡®å®šç¼©æ”¾çº§åˆ«
        # æ›´ç²¾ç¡®çš„ç¼©æ”¾çº§åˆ«æ˜ å°„
        if max_range < 0.001:  # éå¸¸å°çš„èŒƒå›´ï¼ˆçº¦100ç±³ï¼‰
            zoom_level = 18
        elif max_range < 0.005:  # çº¦500ç±³
            zoom_level = 16
        elif max_range < 0.01:  # çº¦1å…¬é‡Œ
            zoom_level = 15
        elif max_range < 0.05:  # çº¦5å…¬é‡Œ
            zoom_level = 14
        elif max_range < 0.1:  # çº¦10å…¬é‡Œ
            zoom_level = 13
        elif max_range < 0.5:  # çº¦50å…¬é‡Œ
            zoom_level = 12
        else:  # å¤§èŒƒå›´
            zoom_level = 10

        # 4. ä½¿ç”¨é«˜åˆ†è¾¨ç‡åœ°å›¾æº
        try:
            # å°è¯•ä½¿ç”¨Stamen TerrainèƒŒæ™¯ï¼Œé€šå¸¸æä¾›è¾ƒé«˜æ¸…æ™°åº¦
            ctx.add_basemap(
                ax,
                crs="EPSG:4326",
                source=ctx.providers.Stamen.Terrain,
                zoom=zoom_level,  # æŒ‡å®šç¼©æ”¾çº§åˆ«
                alpha=0.8,
            )
        except Exception:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨OpenStreetMapä½†æŒ‡å®šç¼©æ”¾çº§åˆ«
            ctx.add_basemap(
                ax,
                crs="EPSG:4326",
                source=ctx.providers.OpenStreetMap.Mapnik,
                zoom=zoom_level,
                alpha=0.8,
            )

        # 5. è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title(f"{scope.capitalize()}ä½ç½®è½¨è¿¹ï¼ˆå¸¦åœ°å›¾åº•å›¾ï¼‰", fontsize=14)
        ax.set_xlabel("ç»åº¦")
        ax.set_ylabel("çº¬åº¦")
        ax.grid(True, alpha=0.3)

        # 6. åªæ˜¾ç¤ºæœ€æ–°6ä¸ªåˆ†æ®µçš„å›¾ä¾‹
        if "segment" in df.columns and len(segments_to_show) > 0:
            ax.legend(
                loc="upper left",
                bbox_to_anchor=(0, 1),
                fontsize="small",
                ncol=min(2, len(segments_to_show)),  # æœ€å¤š2åˆ—
                title="è¡Œç¨‹èµ·å§‹æ—¥æœŸ",  # æ·»åŠ å›¾ä¾‹æ ‡é¢˜
            )

        # 7. æé«˜ä¿å­˜å›¾åƒçš„è´¨é‡
        buf = BytesIO()
        plt.savefig(
            buf,
            format="png",
            dpi=config.DPI,
            bbox_inches="tight",
            facecolor="white",
        )
        plt.close()

        return add_resource_from_bytes(
            buf.getvalue(), title=f"è½¨è¿¹å›¾_{scope}_å¸¦åœ°å›¾.png"
        )

    except ImportError:
        log.warning("æœªå®‰è£…contextilyåº“ï¼Œæ— æ³•æ·»åŠ åœ°å›¾åº•å›¾")
        return generate_trajectory_map_fallback(df, scope, config)


# %% [markdown]
# ### generate_trajectory_map_fallback(df, scope, config)

# %%
def generate_trajectory_map_fallback(df: pd.DataFrame, scope: str, config: Config) -> str:
    """ç”Ÿæˆä¸å¸¦åœ°å›¾åº•å›¾çš„è½¨è¿¹å›¾ï¼ˆå¤‡ç”¨ï¼‰- æ˜¾ç¤ºåˆ†æ®µèµ·å§‹æ—¥æœŸ"""
    plt.figure(figsize=(config.PLOT_WIDTH, config.PLOT_HEIGHT))

    # åªæ˜¾ç¤ºæœ€æ–°6ä¸ªåˆ†æ®µ
    max_legend_items = 6

    if "segment" in df.columns:
        # è·å–æ‰€æœ‰åˆ†æ®µå¹¶æŒ‰èµ·å§‹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        segments = df["segment"].unique()

        # è®¡ç®—æ¯ä¸ªåˆ†æ®µçš„èµ·å§‹æ—¶é—´
        segment_start_time = {}
        for segment in segments:
            seg_df = df[df["segment"] == segment]
            segment_start_time[segment] = seg_df["time"].min()  # ä½¿ç”¨min()è·å–èµ·å§‹æ—¶é—´

        # æŒ‰èµ·å§‹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        sorted_segments = sorted(
            segments, key=lambda x: segment_start_time[x], reverse=True
        )

        # åªä¿ç•™å‰6ä¸ªæœ€æ–°åˆ†æ®µ
        segments_to_show = sorted_segments[:max_legend_items]

        # ç»˜åˆ¶æ‰€æœ‰åˆ†æ®µä½†åªæ˜¾ç¤ºæœ€æ–°6ä¸ªçš„å›¾ä¾‹
        for segment in segments:
            seg_df = df[df["segment"] == segment]
            if segment in segments_to_show:
                # æ ¼å¼åŒ–æ—¥æœŸä¸º"25å¹´9æœˆ1æ—¥"çš„æ ¼å¼
                start_date_str = segment_start_time[segment].strftime("%yå¹´%-mæœˆ%-dæ—¥")

                plt.plot(
                    seg_df["longitude"],
                    seg_df["latitude"],
                    alpha=0.7,
                    linewidth=1.5,
                    label=f"{start_date_str}",
                )
            else:
                plt.plot(
                    seg_df["longitude"],
                    seg_df["latitude"],
                    alpha=0.7,
                    linewidth=1.5,
                    color="gray",
                )

        plt.legend(
            loc="upper left",
            fontsize="small",
            ncol=min(2, len(segments_to_show)),
            title="è¡Œç¨‹èµ·å§‹æ—¥æœŸ",  # æ·»åŠ å›¾ä¾‹æ ‡é¢˜
        )
    else:
        plt.plot(df["longitude"], df["latitude"], "b-", alpha=0.5, linewidth=1)

    plt.title(f"{scope.capitalize()}ä½ç½®è½¨è¿¹")
    plt.xlabel("ç»åº¦")
    plt.ylabel("çº¬åº¦")
    plt.grid(True)

    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=config.DPI)
    plt.close()

    return add_resource_from_bytes(buf.getvalue(), title=f"è½¨è¿¹å›¾_{scope}.png")


# %% [markdown]
# ### generate_stay_points_map(df, scope, config)

# %%
def generate_stay_points_map(df: pd.DataFrame, scope: str, config: Config) -> str:
    """ç”Ÿæˆåœç•™ç‚¹åˆ†å¸ƒå›¾"""
    # import matplotlib.pyplot as plt

    plt.figure(figsize=(config.PLOT_WIDTH, config.PLOT_HEIGHT))

    # ç»˜åˆ¶æ‰€æœ‰è½¨è¿¹ç‚¹
    plt.scatter(
        df["longitude"], df["latitude"], c="gray", alpha=0.3, s=5, label="è½¨è¿¹ç‚¹"
    )

    # çªå‡ºæ˜¾ç¤ºåœç•™ç‚¹
    stay_df = df[df["is_stay"]]
    plt.scatter(
        stay_df["longitude"], stay_df["latitude"], c="red", s=50, label="åœç•™ç‚¹"
    )

    # æ ‡æ³¨é«˜é¢‘åœç•™ç‚¹
    top_stays = stay_df.groupby("cluster").size().nlargest(5).index
    for cluster_id in top_stays:
        cluster_df = stay_df[stay_df["cluster"] == cluster_id]
        center_lon = cluster_df["longitude"].mean()
        center_lat = cluster_df["latitude"].mean()
        # å…ˆç»˜åˆ¶æ ‡è®°ï¼Œå†æ·»åŠ æ–‡æœ¬
        plt.plot(
            center_lon, center_lat, "o", markersize=8, color="red"
        )  # ç»˜åˆ¶ä¸€ä¸ªåœ†ç‚¹æ ‡è®°
        plt.text(
            center_lon,
            center_lat + 0.001,  # ç¨å¾®åç§»ä»¥é¿å…é‡å 
            str(cluster_id),
            fontsize=10,
            ha="center",
            va="bottom",
        )
        # ç»˜åˆ¶Latexå€’ä¸‰è§’å½¢ï¼Œç©ºå¿ƒ
        # plt.text(
        #     center_lon,
        #     center_lat,
        #     r"$\triangledown$" + f"{cluster_id}",
        #     fontsize=12,
        #     ha="center",
        #     va="bottom",
        # )

    plt.title(f"{scope.capitalize()}åœç•™ç‚¹åˆ†å¸ƒ")
    plt.xlabel("ç»åº¦")
    plt.ylabel("çº¬åº¦")
    plt.legend()

    # ä¿å­˜ä¸ºå›¾ç‰‡èµ„æº
    buf = BytesIO()

    plt.savefig(buf, format="png", dpi=config.DPI)
    plt.close()
    return add_resource_from_bytes(buf.getvalue(), f"åœç•™ç‚¹åˆ†å¸ƒ_{scope}.png")


# %% [markdown]
# ### generate_interactive_map(df, scope, config)

# %%
def generate_interactive_map(df: pd.DataFrame, scope: str, config: Config) -> str:
    """ç”Ÿæˆäº¤äº’å¼Leafletåœ°å›¾"""
    import folium

    # åˆ›å»ºåŸºç¡€åœ°å›¾
    center_lat = df["latitude"].mean()
    center_lon = df["longitude"].mean()
    m = folium.Map(location=[center_lat, center_lon], zoom_start=12)

    # æ·»åŠ è½¨è¿¹çº¿
    points = list(zip(df["latitude"], df["longitude"]))
    folium.PolyLine(points, color="blue", weight=2, opacity=0.7).add_to(m)

    # æ·»åŠ åœç•™ç‚¹æ ‡è®°
    stay_df = df[df["is_stay"]]
    for _, row in stay_df.iterrows():
        folium.CircleMarker(
            location=[row["latitude"], row["longitude"]],
            radius=8,
            color="red",
            fill=True,
            popup=f"åœç•™æ—¶é—´: {row.get('duration', 0) / 60:.1f}åˆ†é’Ÿ",
        ).add_to(m)

    # ä¿å­˜ä¸ºhtmlæ–‡ä»¶
    map_path = f"/tmp/interactive_map_{scope}.html"
    m.save(map_path)
    res_id = createresource(map_path, title="äº¤äº’åœ°å›¾.html")
    os.remove(map_path)

    return res_id


# %% [markdown]
# ### generate_time_series_analysis(df, scope, config)

# %%
def generate_time_series_analysis(df: pd.DataFrame, scope: str, config: Config) -> str:
    """ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†æå›¾è¡¨"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

    # æ¯æ—¥è®°å½•æ•°é‡è¶‹åŠ¿
    daily_counts = df.resample("D", on="time").size()
    ax1.plot(daily_counts.index, daily_counts.values)
    ax1.set_title("æ¯æ—¥è®°å½•æ•°é‡è¶‹åŠ¿")
    ax1.tick_params(axis="x", rotation=45)

    # å‘¨å†…åˆ†å¸ƒçƒ­åŠ›å›¾
    df["weekday"] = df["time"].dt.dayofweek
    df["hour"] = df["time"].dt.hour
    weekday_hour = df.groupby(["weekday", "hour"]).size().unstack()
    sns.heatmap(weekday_hour, ax=ax2, cmap="YlOrRd")
    ax2.set_title("å‘¨å†…æ—¶é—´åˆ†å¸ƒçƒ­åŠ›å›¾")

    # ç§»åŠ¨é€Ÿåº¦åˆ†æï¼ˆå¦‚æœæœ‰æ—¶é—´å·®å’Œè·ç¦»æ•°æ®ï¼‰
    if "dist_to_prev" in df.columns and "time_diff" in df.columns:
        df["speed"] = df["dist_to_prev"] / (df["time_diff"] / 3600)  # km/h
        valid_speeds = df["speed"][np.isfinite(df["speed"])]  # è¿‡æ»¤inf/nan

        if not valid_speeds.empty:
            ax3.hist(valid_speeds, bins=50, alpha=0.7)
            ax3.set_title("é€Ÿåº¦åˆ†å¸ƒ")
            ax3.set_xlabel("é€Ÿåº¦ (å…¬é‡Œ/å°æ—¶)")
            ax3.set_ylabel("æ•°é‡")
        else:
            ax3.text(
                0.5,
                0.5,
                "æ— æœ‰æ•ˆé€Ÿåº¦æ•°æ®",
                horizontalalignment="center",
                verticalalignment="center",
                transform=ax3.transAxes,
            )
            ax3.set_title("é€Ÿåº¦åˆ†å¸ƒ (æ— æ•°æ®)")

    # è®°å½•é—´éš”åˆ†å¸ƒ
    if "time_diff" in df.columns:
        ax4.hist(df["time_diff"].dropna(), bins=50, alpha=0.7)
        ax4.set_title("è®°å½•æ—¶é—´é—´éš”åˆ†å¸ƒ")
        ax4.set_xlabel("æ—¶é—´é—´éš” (åˆ†é’Ÿ)")

    plt.tight_layout()
    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=config.DPI)
    plt.close()

    return add_resource_from_bytes(buf.getvalue(), f"æ—¶é—´åºåˆ—åˆ†æ_{scope}.png")


# %% [markdown]
# ### enhanced_stay_points_analysis(df, scope, config)

# %%
def enhanced_stay_points_analysis(df: pd.DataFrame, scope: str, config: Config) -> str:
    """å¢å¼ºç‰ˆåœç•™ç‚¹åˆ†æ"""
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))

    # åœç•™æ—¶é•¿åˆ†å¸ƒ
    stay_durations = df[df["is_stay"]]["duration"] / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ
    ax1.hist(stay_durations, bins=30, alpha=0.7, color="skyblue")
    ax1.set_title("åœç•™æ—¶é•¿åˆ†å¸ƒ")
    ax1.set_xlabel("åœç•™æ—¶é—´ (åˆ†é’Ÿ)")
    ax1.set_ylabel("é¢‘æ¬¡")

    # åœç•™ç‚¹è®¿é—®é¢‘æ¬¡
    stay_counts = df[df["is_stay"]].groupby("cluster").size()
    ax2.bar(range(len(stay_counts)), sorted(stay_counts.values, reverse=True))
    ax2.set_title("åœç•™ç‚¹è®¿é—®é¢‘æ¬¡æ’å")
    ax2.set_xlabel("åœç•™ç‚¹æ’å")
    ax2.set_ylabel("è®¿é—®æ¬¡æ•°")

    # åœç•™ç‚¹æ—¶é—´åˆ†å¸ƒï¼ˆæ—¥/å¤œï¼‰
    if "hour" in df.columns:
        day_stays = df[df["is_stay"] & (df["hour"].between(6, 18))]
        night_stays = df[df["is_stay"] & (~df["hour"].between(6, 18))]
        ax3.bar(
            ["ç™½å¤©", "å¤œæ™š"],
            [len(day_stays), len(night_stays)],
            color=["orange", "navy"],
        )
        ax3.set_title("åœç•™ç‚¹æ—¶é—´åˆ†å¸ƒ")

    plt.tight_layout()
    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=config.DPI)
    plt.close()

    return add_resource_from_bytes(buf.getvalue(), f"å¢å¼ºåœç•™ç‚¹åˆ†æ_{scope}.png")


# %% [markdown]
# ### data_quality_dashboard(df, scope, config)

# %%
def data_quality_dashboard(df: pd.DataFrame, scope: str, config: Config) -> str:
    """æ•°æ®è´¨é‡ç›‘æ§ä»ªè¡¨æ¿"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # æ•°æ®å®Œæ•´æ€§æ—¶é—´åºåˆ—
    daily_completeness = (
        df.resample("D", on="time").count()["latitude"] / 1440
    )  # å‡è®¾å®Œæ•´æ•°æ®ä¸º1440æ¡/å¤©
    axes[0, 0].plot(daily_completeness.index, daily_completeness.values)
    axes[0, 0].set_title("æ¯æ—¥æ•°æ®å®Œæ•´æ€§")
    axes[0, 0].set_ylim(0, 1)

    # ç²¾åº¦éšæ—¶é—´å˜åŒ–
    if "accuracy" in df.columns:
        daily_accuracy = df.resample("D", on="time")["accuracy"].mean()
        axes[0, 1].plot(daily_accuracy.index, daily_accuracy.values)
        axes[0, 1].set_title("æ—¥å‡å®šä½ç²¾åº¦")
        axes[0, 1].set_ylabel("ç²¾åº¦ (ç±³)")

    # è®¾å¤‡æ•°æ®è´¡çŒ®æ¯”ä¾‹
    device_contrib = df["device_id"].value_counts()
    axes[1, 0].pie(
        device_contrib.values, labels=device_contrib.index, autopct="%1.1f%%"
    )
    axes[1, 0].set_title("è®¾å¤‡æ•°æ®è´¡çŒ®æ¯”ä¾‹")

    # æ—¶é—´é—´éš”åˆ†å¸ƒ
    if "time_diff" in df.columns:
        axes[1, 1].hist(df["time_diff"].dropna(), bins=50, alpha=0.7)
        axes[1, 1].set_title("è®°å½•æ—¶é—´é—´éš”åˆ†å¸ƒ")
        axes[1, 1].set_xlabel("æ—¶é—´é—´éš” (åˆ†é’Ÿ)")

    plt.tight_layout()
    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=config.DPI)
    plt.close()

    return add_resource_from_bytes(buf.getvalue(), f"æ•°æ®è´¨é‡ä»ªè¡¨æ¿_{scope}.png")


# %% [markdown]
# ### movement_pattern_analysis(df, scope)

# %%
def movement_pattern_analysis(df: pd.DataFrame, scope: str, config: Config) -> str:
    """ç§»åŠ¨æ¨¡å¼è¯†åˆ«å’Œåˆ†æ"""
    # ä½¿ç”¨èšç±»ç®—æ³•è¯†åˆ«ç§»åŠ¨æ¨¡å¼
    from sklearn.cluster import KMeans

    # æå–ç§»åŠ¨ç‰¹å¾ï¼šé€Ÿåº¦ã€æ–¹å‘å˜åŒ–ç­‰
    movement_features = []
    for i in range(1, len(df)):
        point1 = df.iloc[i - 1]
        point2 = df.iloc[i]

        # è®¡ç®—ç§»åŠ¨ç‰¹å¾
        distance = great_circle(
            (point1["latitude"], point1["longitude"]),
            (point2["latitude"], point2["longitude"]),
        ).km

        time_diff = (point2["time"] - point1["time"]).total_seconds() / 3600
        speed = distance / time_diff if time_diff > 0 else 0

        movement_features.append([distance, speed])

    # èšç±»åˆ†æ
    kmeans = KMeans(n_clusters=3, random_state=42)
    clusters = kmeans.fit_predict(movement_features)

    # å¯è§†åŒ–èšç±»ç»“æœ
    plt.figure(figsize=(10, 6))
    scatter = plt.scatter(
        [f[0] for f in movement_features],
        [f[1] for f in movement_features],
        c=clusters,
        cmap="viridis",
        alpha=0.6,
    )
    plt.colorbar(scatter)
    plt.title("ç§»åŠ¨æ¨¡å¼èšç±»åˆ†æ")
    plt.xlabel("ç§»åŠ¨è·ç¦» (km)")
    plt.ylabel("ç§»åŠ¨é€Ÿåº¦ (km/h)")

    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=config.DPI)
    plt.close()

    return add_resource_from_bytes(buf.getvalue(), f"ç§»åŠ¨æ¨¡å¼åˆ†æ_{scope}.png")

# %% [markdown]
# ### `generate_visualizations(df, analysis_results)`
# ç”Ÿæˆä½ç½®æ•°æ®çš„å¯è§†åŒ–å›¾è¡¨


# %%
def generate_visualizations(analysis_results: str, scope: str) -> dict:
    """ä»åˆ†æç»“æœä¸­æå–å¯è§†åŒ–èµ„æºID"""
    # ç›´æ¥è¿”å› analysis_results ä¸­çš„ resource_ids
    return analysis_results.get("resource_ids", {})


# %% [markdown]
# ## æ„å»ºæŠ¥å‘Šå†…å®¹

# %% [markdown]
# ### `build_report_content(analysis_results, resource_ids, scope)`
# æ„å»ºMarkdownæŠ¥å‘Šå†…å®¹


# %%
def build_report_content(analysis_results: dict, resource_ids:str, scope: str) -> str:
    """æ„å»ºMarkdownæŠ¥å‘Šå†…å®¹"""
    # ä½¿ç”¨ analysis_results å’Œ resource_ids æ„å»ºæŠ¥å‘Š
    content = f"""
# ğŸ“ {scope.capitalize()}ä½ç½®åˆ†ææŠ¥å‘Š
**{analysis_results["time_range"][0]} è‡³ {analysis_results["time_range"][1]}**

## ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡
| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |
|------|----|------|
| **æ€»è®°å½•** | {analysis_results["total_points"]} | ä½ç½®ç‚¹æ•°é‡ |
| **è¦†ç›–å¤©æ•°** | {analysis_results["unique_days"]} | æ•°æ®å®Œæ•´åº¦ |
| **æ´»åŠ¨åŠå¾„** | {analysis_results["distance_km"]:.2f}km | æœ€å¤§ç§»åŠ¨è·ç¦» |
| **æ—¶é—´æ–­å±‚** | {analysis_results["gap_stats"]["count"]} | æœ€é•¿é—´éš” {analysis_results["gap_stats"]["longest_gap"]:.1f}h |

## ğŸ“± è®¾å¤‡åˆ†å¸ƒ
![è®¾å¤‡åˆ†å¸ƒ](:/{resource_ids["device_dist"]})

## ğŸ¯ å®šä½ç²¾åº¦
| æŒ‡æ ‡ | å€¼ |
|------|----|
| **æœ€ä½³ç²¾åº¦** | {analysis_results["accuracy_stats"]["min"]:.1f}m |
| **æœ€å·®ç²¾åº¦** | {analysis_results["accuracy_stats"]["max"]:.1f}m |
| **å¹³å‡ç²¾åº¦** | {analysis_results["accuracy_stats"]["mean"]:.1f}m |

## ğŸ•’ æ—¶é—´åˆ†å¸ƒ
![æ—¶é—´åˆ†å¸ƒ](:/{resource_ids["time_heatmap"]})

## ğŸ›‘ åœç•™ç‚¹åˆ†æ
| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |
|---|---|---|
| æ€»åœç•™æ¬¡æ•° | {analysis_results["stay_stats"]["total_stays"]} | è¯†åˆ«åˆ°çš„åœç•™ç‚¹æ•°é‡ |
| å¹³å‡åœç•™æ—¶é•¿ | {analysis_results["stay_stats"]["avg_duration"]:.1f}åˆ†é’Ÿ | æ¯æ¬¡åœç•™çš„å¹³å‡æ—¶é—´ |
| é«˜é¢‘åœç•™ç‚¹ | {len(analysis_results["stay_stats"]["top_locations"])}å¤„ | è®¿é—®æœ€é¢‘ç¹çš„åœ°ç‚¹ |

### åœç•™ç‚¹åˆ†å¸ƒå›¾
![åœç•™ç‚¹åˆ†å¸ƒ](:/{resource_ids["stay_points_map"]})

## ğŸŒ å…³é”®åœ°ç‚¹
| ä½ç½® | è®¿é—® | åœç•™ | åæ ‡ |
|------|------|------|------|
"""
    for i, place in enumerate(analysis_results["important_places"][:3]):
        visit_count = int(place["visit_count"])
        lat = place["latitude"]
        lon = place["longitude"]
        content += f"""| **åœ°ç‚¹{i + 1}** | {visit_count}æ¬¡ | {place["avg_stay_min"]:.1f}åˆ† | [{lat}, {lon}]({generate_geo_link(lat, lon)}) |\n"""

    content += f"""
## ğŸ“ˆ ç©ºé—´åˆ†æ
### ç§»åŠ¨è½¨è¿¹
![ç§»åŠ¨è½¨è¿¹](:/{resource_ids["trajectory_with_map"]})

### ä½ç½®ç²¾åº¦åˆ†å¸ƒ
![ä½ç½®ç²¾åº¦åˆ†å¸ƒ](:/{resource_ids["accuracy"]})

## ğŸ—ºï¸ äº¤äº’å¼åœ°å›¾
[æŸ¥çœ‹äº¤äº’å¼åœ°å›¾](:/{resource_ids["interactive_map"]})

## ğŸ“ˆ æ—¶é—´æ¨¡å¼åˆ†æ
![æ—¶é—´åºåˆ—åˆ†æ](:/{resource_ids["time_series"]})

## ğŸ›‘ æ·±åº¦åœç•™åˆ†æ
![åœç•™ç‚¹åˆ†æ](:/{resource_ids["enhanced_stays"]})

## ğŸš¶ ç§»åŠ¨æ¨¡å¼è¯†åˆ«
![ç§»åŠ¨æ¨¡å¼](:/{resource_ids["movement_patterns"]})

## âœ… æ•°æ®è´¨é‡ç›‘æ§
![æ•°æ®è´¨é‡](:/{resource_ids["data_quality"]})
"""
    return content


# %% [markdown]
# ## æ›´æ–°Joplinç¬”è®°

# %% [markdown]
# ### `update_joplin_report(report_content, scope)`
# æ›´æ–°Joplinä½ç½®åˆ†ææŠ¥å‘Š


# %%
def update_joplin_report(report_content: str, scope: str) -> None:
    """æ›´æ–°Joplinä½ç½®åˆ†ææŠ¥å‘Š"""
    note_title = f"ä½ç½®åˆ†ææŠ¥å‘Š_{scope}"
    existing_notes = searchnotes(f"{note_title}")

    if existing_notes:
        note_id = existing_notes[0].id
        # æ›´æ–°ç¬”è®°å†…å®¹
        updatenote_body(note_id, report_content)
    else:
        parent_id = searchnotebook("ewmobile")
        if not parent_id:
            parent_id = createnote(title="ewmobile", notebook=True)

        # åˆ›å»ºæ–°ç¬”è®°
        note_id = createnote(title=note_title, parent_id=parent_id, body=report_content)


# %% [markdown]
# ## ä¸»å‡½æ•°

# %% [markdown]
# ### `generate_location_reports()`
# ç”Ÿæˆä¸‰ä¸ªå±‚çº§çš„æŠ¥å‘Šï¼šæœˆæŠ¥ã€å­£æŠ¥ã€å¹´æŠ¥


# %%
@timethis
def generate_location_reports(config: Config) -> None:
    """ç”Ÿæˆä¸‰ä¸ªå±‚çº§çš„æŠ¥å‘Šï¼šæœˆæŠ¥ã€å­£æŠ¥ã€å¹´æŠ¥"""
    for scope in list(config.REPORT_LEVELS.keys())[:]:
        log.info(f"å¼€å§‹ç”Ÿæˆ {scope} ä½ç½®æŠ¥å‘Š...")

        # 1. åŠ è½½æ•°æ®
        df = load_location_data(scope, config)
        if df.empty:
            log.warning(f"è·³è¿‡ {scope} æŠ¥å‘Šï¼Œæ— æ•°æ®")
            continue

        # 2. åˆ†ææ•°æ®å¹¶ç”Ÿæˆå¯è§†åŒ–èµ„æº
        analysis_results = analyze_location_data(df, scope)

        # 3. ä»åˆ†æç»“æœä¸­è·å–èµ„æºID
        resource_ids = generate_visualizations(analysis_results, scope)

        # 4. æ„å»ºæŠ¥å‘Š
        report_content = build_report_content(analysis_results, resource_ids, scope)

        # 5. æ›´æ–°ç¬”è®°
        update_joplin_report(report_content, scope)


# %% [markdown]
# ## ä¸»å…¥å£

# %% [markdown]
# ### `main()`
# è„šæœ¬ä¸»å…¥å£

# %%
if __name__ == "__main__":
    log.info("å¼€å§‹ç”Ÿæˆä½ç½®åˆ†ææŠ¥å‘Š...")
    generate_location_reports(Config())
    log.info("ä½ç½®åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
