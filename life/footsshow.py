# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     cell_metadata_filter: -all
#     formats: ipynb,py:percent
#     notebook_metadata_filter: jupytext,-kernelspec,-jupytext.text_representation.jupytext_version
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
# ---

# %% [markdown]
# # ä½ç½®æ•°æ®å±•ç¤ºä¸åˆ†æç³»ç»Ÿ
#
# ## åŠŸèƒ½ï¼šä»JoplinåŠ è½½è§„æ•´ä½ç½®æ•°æ®ï¼Œç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š

# %%
import base64
import os
import re
from datetime import datetime, timedelta
from io import BytesIO
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# %%
import pathmagic
from geopy.distance import great_circle
from sklearn.cluster import DBSCAN

with pathmagic.context():
    from func.configpr import getcfpoptionvalue
    from func.first import getdirmain
    from func.jpfuncs import (
        add_resource_from_bytes,
        createnote,
        getinivaluefromcloud,
        jpapi,
        searchnotes,
        updatenote_body,
    )
    from func.logme import log

# %% [markdown]
# ## é…ç½®å‚æ•°

# %%
# æŠ¥å‘Šå±‚çº§
REPORT_LEVELS = {"monthly": 1, "quarterly": 3, "yearly": 12}

# å¯è§†åŒ–å‚æ•°
PLOT_WIDTH = 10
PLOT_HEIGHT = 8
DPI = 150

# %% [markdown]
# ## æ•°æ®åŠ è½½å‡½æ•°

# %% [markdown]
# ### `load_location_data(scope)`
# åŠ è½½æŒ‡å®šèŒƒå›´çš„ä½ç½®æ•°æ®


# %%
def load_location_data(scope):
    """
    åŠ è½½æŒ‡å®šèŒƒå›´çš„ä½ç½®æ•°æ®
    """
    end_date = datetime.now()
    months = REPORT_LEVELS[scope]
    start_date = end_date - timedelta(days=30 * months)

    date_range = pd.date_range(start_date, end_date, freq="MS")
    monthly_dfs = []

    for date in date_range:
        month_str = date.strftime("%Y%m")
        note_title = f"ä½ç½®æ•°æ®_{month_str}"
        notes = searchnotes(f"title:{note_title}")

        if not notes:
            log.warning(f"æœªæ‰¾åˆ°{month_str}çš„ä½ç½®æ•°æ®ç¬”è®°")
            continue

        note = notes[0]
        resources = jpapi.get_resources(note.id).items

        location_resource = None
        for res in resources:
            if res.title.endswith(".xlsx"):
                location_resource = res
                break

        if not location_resource:
            log.warning(f"æœªæ‰¾åˆ°{month_str}çš„ä½ç½®æ•°æ®é™„ä»¶")
            continue

        res_data = jpapi.get_resource_file(location_resource.id)
        df = pd.read_excel(BytesIO(res_data))
        df["month"] = month_str
        monthly_dfs.append(df)

    if not monthly_dfs:
        log.warning(f"æœªæ‰¾åˆ°{scope}çš„ä½ç½®æ•°æ®")
        return pd.DataFrame()

    return pd.concat(monthly_dfs).reset_index(drop=True)


# %% [markdown]
# ## æ•°æ®åˆ†æå‡½æ•°

# %% [markdown]
# ### `analyze_location_data(df, scope)`
# åˆ†æä½ç½®æ•°æ®ï¼Œè¿”å›ç»Ÿè®¡ç»“æœ


# %%
def analyze_location_data(indf, scope):
    """
    åˆ†æä½ç½®æ•°æ®ï¼Œè¿”å›ç»Ÿè®¡ç»“æœ
    ä¿®å¤åˆ—åé—®é¢˜å¹¶æ·»åŠ æ•°æ®é¢„å¤„ç†
    """
    # 1. æ•°æ®é¢„å¤„ç†
    df = indf.copy()
    df = fuse_device_data(df)
    df = handle_time_jumps(df)
    # ç¡®ä¿æ—¶é—´æˆ³å’Œæ—¶é—´å·®åˆ—å­˜åœ¨
    df["timestamp"] = df["time"].dt.strftime("%Y-%m-%d %H:%M:%S")
    df["time_diff"] = df["time"].diff().dt.total_seconds().fillna(0) / 60

    # 2. è®°å½•è°ƒè¯•ä¿¡æ¯
    log.debug(f"åˆ†æå¯åŠ¨æ—¶æ•°æ®åˆ—ä¸º: {df.columns.tolist()}")

    # 3. æ—¶é—´èŒƒå›´åˆ†æ
    start_time = df["time"].min().strftime("%Y-%m-%d")
    end_time = df["time"].max().strftime("%Y-%m-%d")

    # 4. åŸºæœ¬ç»Ÿè®¡
    total_points = len(df)
    unique_days = df["time"].dt.date.nunique()

    # 5. è®¾å¤‡åˆ†æ
    device_stats = df["device_id"].value_counts().to_dict()

    # 6. è·ç¦»
    min_lat, max_lat = df["latitude"].min(), df["latitude"].max()
    min_lon, max_lon = df["longitude"].min(), df["longitude"].max()
    distance_km = great_circle((min_lat, min_lon), (max_lat, max_lon)).kilometers
    # 7. å¤§è·¨è¶Š
    if "big_gap" in df.columns:
        big_gaps = df[df["big_gap"]]
        gap_stats = {
            "count": len(big_gaps),
            "longest_gap": df["time_diff"].max() if "time_diff" in df.columns else 0,
        }
    else:
        gap_stats = {"count": 0, "longest_gap": 0}

    # 8. å°æ—¶åˆ†å¸ƒ
    df["hour"] = df["time"].dt.hour
    hourly_distribution = df["hour"].value_counts().sort_index().to_dict()

    # 9. ç²¾åº¦åˆ†æ
    accuracy_stats = {
        "min": df["accuracy"].min(),
        "max": df["accuracy"].max(),
        "mean": df["accuracy"].mean(),
    }

    # 10. é‡è¦åœ°ç‚¹åˆ†æ
    clustered = identify_important_places(df)
    important_places = (
        clustered.groupby("cluster")
        .agg(
            {
                "latitude": "mean",
                "longitude": "mean",
            }
        )
        .assign(visit_count=1)
        .assign(avg_stay_min=0)
        .sort_values("visit_count", ascending=False)
        .head(5)
    )

    log.debug(f"åˆ†æç»“æŸæ—¶æ•°æ®åˆ—ä¸º: {df.columns.tolist()}")
    return {
        "scope": scope,
        "time_range": (start_time, end_time),
        "total_points": total_points,
        "unique_days": unique_days,
        "device_stats": device_stats,
        "distance_km": distance_km,
        "gap_stats": gap_stats,
        "accuracy_stats": accuracy_stats,
        "hourly_distribution": hourly_distribution,
        "important_places": important_places.to_dict("records"),
    }


# %% [markdown]
# ### `handle_time_jumps(df)`


# %%
def handle_time_jumps(df):
    if df.empty:
        return df

    df = df.sort_values("time")
    df["time_diff"] = df["time"].diff().dt.total_seconds() / 60
    df["big_gap"] = df["time_diff"] > 4 * 60
    df["segment"] = df["big_gap"].cumsum()

    return df


# %% [markdown]
# ### `calc_device_activity(df, device_id)`


# %%
def calc_device_activity(df, device_id):
    """è®¡ç®—è®¾å¤‡æ´»è·ƒåº¦è¯„åˆ†ï¼ˆ0-100ï¼‰"""
    device_data = df[df["device_id"] == device_id]
    if len(device_data) < 2:
        return 0

    total_dist = 0
    prev = None
    for _, row in device_data.iterrows():
        if prev is not None:
            dist = great_circle(
                (prev.latitude, prev.longitude), (row.latitude, row.longitude)
            ).m
            total_dist += dist
        prev = row

    time_span = (
        device_data["time"].max() - device_data["time"].min()
    ).total_seconds() / 3600
    lat_var = device_data["latitude"].var()
    lon_var = device_data["longitude"].var()

    activity_score = min(
        100,
        (total_dist / max(1, time_span)) * 0.7 + (lat_var + lon_var) * 10000 * 0.3,
    )
    return activity_score


# %% [markdown]
# ### `check_spatiotemporal_consistency(point1, point2)`


# %%
def check_spatiotemporal_consistency(point1, point2):
    """æ£€æŸ¥ä¸¤ç‚¹æ—¶ç©ºä¸€è‡´æ€§"""
    time_diff = abs((point1["time"] - point2["time"]).total_seconds())
    dist = great_circle(
        (point1.latitude, point1.longitude), (point2.latitude, point2.longitude)
    ).m
    max_allowed_dist = min(100, time_diff * 0.5)  # 0.5m/sç§»åŠ¨é€Ÿåº¦
    return dist < max_allowed_dist and time_diff < 300


# %% [markdown]
# ### `fuse_device_data(df, window_size="2h")`


# %%
def fuse_device_data(df, window_size="2h"):
    """å¤šè®¾å¤‡æ•°æ®æ™ºèƒ½èåˆ"""
    print(
        f"å¼€å§‹å¤šè®¾å¤‡æ•°æ®æ™ºèƒ½èåˆâ€¦â€¦\nä¼ å…¥æ±‡æ€»æ•°æ®å¤§å°ä¸ºï¼š{df.shape[0]}ï¼Œä¼ å…¥çš„åˆ—åç§°åˆ—è¡¨ä¸ºï¼š{list(df.columns)}"
    )
    device_activity = {
        device_id: calc_device_activity(df, device_id)
        for device_id in df["device_id"].unique()
    }
    df["time_window"] = df["time"].dt.floor(window_size)
    fused_points = []

    for window, group in df.groupby("time_window"):
        active_devices = [
            did
            for did, score in device_activity.items()
            if score > 50 and did in group["device_id"].values
        ]
        if active_devices:
            active_group = group[group["device_id"].isin(active_devices)]
            candidate = active_group.loc[active_group["accuracy"].idxmin()]
        else:
            candidate = group.loc[group["accuracy"].idxmin()]

        if fused_points:
            last_point = fused_points[-1]
            if not check_spatiotemporal_consistency(last_point, candidate):
                group["dist_to_last"] = group.apply(
                    lambda row: great_circle(
                        (last_point.latitude, last_point.longitude),
                        (row.latitude, row.longitude),
                    ).m,
                    axis=1,
                )
                candidate = group.loc[group["dist_to_last"].idxmin()]

        fused_points.append(candidate)
    outdf = pd.DataFrame(fused_points)
    print(
        f"æŒ‰ç…§æ—¶é—´çª—å£{window_size}åˆ¤æ–­å¹¶å¤„ç†æ´»è·ƒè®¾å¤‡ï¼Œæ•´åˆå®Œæ¯•æ•°æ®å¤§å°ä¸º{outdf.shape[0]}ï¼Œåˆ—åç§°åˆ—è¡¨ä¸ºï¼š{list(outdf.columns)}"
    )

    return outdf


# %% [markdown]
# ### `detect_static_devices(df, var_threshold=0.00001)`


# %%
def detect_static_devices(df, var_threshold=0.00001):
    """è¯†åˆ«å¹¶è¿‡æ»¤é™æ€è®¾å¤‡"""
    static_devices = []
    for device_id, device_data in df.groupby("device_id"):
        lat_var = device_data["latitude"].var()
        lon_var = device_data["longitude"].var()

        if lat_var < var_threshold and lon_var < var_threshold:
            static_devices.append(device_id)
            log.info(f"è®¾å¤‡ {device_id} è¢«è¯†åˆ«ä¸ºé™æ€è®¾å¤‡")

    return df[~df["device_id"].isin(static_devices)]


# %% [markdown]
# ## é‡è¦åœ°ç‚¹è¯†åˆ«

# %% [markdown]
# ### `identify_important_places(df, radius_km=0.5, min_points=3)`
# è¯†åˆ«é‡è¦åœ°ç‚¹ï¼ˆåœç•™ç‚¹ï¼‰


# %%
def identify_important_places(df, radius_km=0.5, min_points=3):
    """
    è¯†åˆ«é‡è¦åœ°ç‚¹ï¼ˆåœç•™ç‚¹ï¼‰
    """
    required_cols = ["latitude", "longitude", "time_diff"]
    if not all(col in df.columns for col in required_cols):
        return pd.DataFrame()

    coords = df[["latitude", "longitude"]].values
    kms_per_radian = 6371.0088
    epsilon = radius_km / kms_per_radian

    db = DBSCAN(
        eps=epsilon, min_samples=min_points, algorithm="ball_tree", metric="haversine"
    ).fit(np.radians(coords))
    df["cluster"] = db.labels_

    clustered = df[df["cluster"] != -1]

    if clustered.empty:
        return pd.DataFrame()

    cluster_centers = (
        clustered.groupby("cluster")
        .agg({"latitude": "mean", "longitude": "mean", "time": "count"})
        .rename(columns={"time": "visit_count"})
        .reset_index()
    )

    cluster_centers["avg_stay_min"] = (
        clustered.groupby("cluster")["time_diff"].mean().values
    )
    print(cluster_centers.columns)
    return cluster_centers.sort_values("visit_count", ascending=False).head(10)


# %%
# åœ¨footsshow.pyä¸­ä¿®æ”¹
def generate_visualizations(df, analysis_results, scope):
    """ç”Ÿæˆä½ç½®æ•°æ®çš„å¯è§†åŒ–å›¾è¡¨å¹¶è¿”å›èµ„æºID"""
    resource_ids = {}

    # 1. è½¨è¿¹å›¾
    plt.figure(figsize=(PLOT_WIDTH, PLOT_HEIGHT))
    if "segment" in df.columns:
        for segment in df["segment"].unique():
            seg_df = df[df["segment"] == segment]
            plt.plot(
                seg_df["longitude"],
                seg_df["latitude"],
                alpha=0.7,
                linewidth=1.5,
                label=f"æ®µ {segment}",
            )
    else:
        plt.plot(df["longitude"], df["latitude"], "b-", alpha=0.5, linewidth=1)

    plt.title(f"{scope.capitalize()}ä½ç½®è½¨è¿¹")
    plt.xlabel("ç»åº¦")
    plt.ylabel("çº¬åº¦")
    plt.grid(True)
    plt.legend(loc="best")

    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=DPI)
    plt.close()
    resource_ids["trajectory"] = add_resource_from_bytes(
        buf.getvalue(), title=f"è½¨è¿¹å›¾_{scope}.png"
    )

    # 2. æ—¶é—´åˆ†å¸ƒå›¾
    plt.figure(figsize=(PLOT_WIDTH, PLOT_HEIGHT - 2))
    hourly_distribution = analysis_results["hourly_distribution"]
    plt.bar(
        list(hourly_distribution.keys()), list(hourly_distribution.values()), width=0.8
    )
    plt.title(f"{scope.capitalize()}ä½ç½®è®°å½•æ—¶é—´åˆ†å¸ƒ")
    plt.xlabel("å°æ—¶")
    plt.ylabel("è®°å½•æ•°é‡")
    plt.xticks(range(0, 24))
    plt.grid(True)

    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=DPI)
    plt.close()
    resource_ids["time_dist"] = add_resource_from_bytes(
        buf.getvalue(), title=f"æ—¶é—´åˆ†å¸ƒ_{scope}.png"
    )

    # 3. ç²¾åº¦åˆ†å¸ƒå›¾
    if "accuracy" in df.columns:
        plt.figure(figsize=(PLOT_WIDTH, PLOT_HEIGHT - 2))
        plt.hist(df["accuracy"].dropna(), bins=50, alpha=0.7)
        plt.title(f"{scope.capitalize()}ä½ç½®ç²¾åº¦åˆ†å¸ƒ")
        plt.xlabel("ç²¾åº¦ (ç±³)")
        plt.ylabel("è®°å½•æ•°é‡")
        plt.grid(True)

        buf = BytesIO()
        plt.savefig(buf, format="png", dpi=DPI)
        plt.close()
        resource_ids["accuracy"] = add_resource_from_bytes(
            buf.getvalue(), title=f"ç²¾åº¦åˆ†å¸ƒ_{scope}.png"
        )

    return resource_ids


# %% [markdown]
# ### generate_device_pie_chart(device_stats)

# %%
def generate_device_pie_chart(device_stats):
    """ç”Ÿæˆè®¾å¤‡åˆ†å¸ƒé¥¼å›¾"""
    plt.figure(figsize=(6, 6))
    # labels = [f"è®¾å¤‡{i + 1}" for i in range(len(device_stats))]
    labels = [
        getinivaluefromcloud("device", str(device_id)) for device_id in device_stats
    ]
    sizes = list(device_stats.values())
    plt.pie(sizes, labels=labels, autopct="%1.1f%%", startangle=90)
    plt.axis("equal")

    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=120)
    plt.close()
    return add_resource_from_bytes(buf.getvalue(), "è®¾å¤‡åˆ†å¸ƒ.png")


# %% [markdown]
# ### generate_time_heatmap(hourly_distribution)

# %%
def generate_time_heatmap(hourly_distribution):
    """ç”Ÿæˆ24å°æ—¶çƒ­åŠ›å›¾"""
    hours = list(range(24))
    values = [hourly_distribution.get(h, 0) for h in hours]

    plt.figure(figsize=(10, 3))
    plt.bar(hours, values, color="#4c72b0")
    plt.xticks(hours)
    plt.xlabel("å°æ—¶")
    plt.ylabel("è®°å½•æ•°")
    plt.grid(axis="y", alpha=0.3)

    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=120)
    plt.close()
    return add_resource_from_bytes(buf.getvalue(), "æ—¶é—´åˆ†å¸ƒçƒ­åŠ›å›¾.png")


# %% [markdown]
# ### generate_geo_link(lat, lon)

# %%
def generate_geo_link(lat, lon):
    """ç”Ÿæˆåœ°å›¾é“¾æ¥"""
    return f" https://www.openstreetmap.org/?mlat= {lat}&mlon={lon}&zoom=15"

# %% [markdown]
# ## å¯è§†åŒ–å‡½æ•°

# %% [markdown]
# ### `generate_visualizations(df, analysis_results)`
# ç”Ÿæˆä½ç½®æ•°æ®çš„å¯è§†åŒ–å›¾è¡¨


# %%
def generate_visualizations(df, analysis_results, scope):
    """ç”Ÿæˆä½ç½®æ•°æ®çš„å¯è§†åŒ–å›¾è¡¨å¹¶è¿”å›èµ„æºID"""
    resource_ids = {}

    # 1. è½¨è¿¹å›¾
    plt.figure(figsize=(PLOT_WIDTH, PLOT_HEIGHT))
    if "segment" in df.columns:
        for segment in df["segment"].unique():
            seg_df = df[df["segment"] == segment]
            plt.plot(
                seg_df["longitude"],
                seg_df["latitude"],
                alpha=0.7,
                linewidth=1.5,
                label=f"æ®µ {segment}",
            )
    else:
        plt.plot(df["longitude"], df["latitude"], "b-", alpha=0.5, linewidth=1)

    plt.title(f"{scope.capitalize()}ä½ç½®è½¨è¿¹")
    plt.xlabel("ç»åº¦")
    plt.ylabel("çº¬åº¦")
    plt.grid(True)
    plt.legend(loc="best")

    buf = BytesIO()
    plt.savefig(buf, format="png", dpi=DPI)
    plt.close()
    resource_ids["trajectory"] = add_resource_from_bytes(
        buf.getvalue(), title=f"è½¨è¿¹å›¾_{scope}.png"
    )
    # 2. æ–°å¢è®¾å¤‡åˆ†å¸ƒé¥¼å›¾
    resource_ids["device_dist"] = generate_device_pie_chart(
        analysis_results["device_stats"]
    )

    # 3. æ–°å¢æ—¶é—´åˆ†å¸ƒçƒ­åŠ›å›¾
    resource_ids["time_heatmap"] = generate_time_heatmap(
        analysis_results["hourly_distribution"]
    )

    # 4. ç²¾åº¦åˆ†å¸ƒå›¾ï¼ˆä¼˜åŒ–å±•ç¤ºï¼‰
    if "accuracy" in df.columns:
        plt.figure(figsize=(PLOT_WIDTH, PLOT_HEIGHT - 2))
        sns.histplot(df["accuracy"].dropna(), bins=30, kde=True, color="#55a868")
        plt.title(f"{scope.capitalize()}å®šä½ç²¾åº¦åˆ†å¸ƒ")
        plt.xlabel("ç²¾åº¦ (ç±³)")
        plt.grid(True, alpha=0.3)
        resource_ids["accuracy"] = add_resource_from_bytes(
            buf.getvalue(), f"ç²¾åº¦åˆ†å¸ƒ_{scope}.png"
        )

    return resource_ids


# %% [markdown]
# ## æ„å»ºæŠ¥å‘Šå†…å®¹

# %% [markdown]
# ### `build_report_content(analysis_results, resource_ids, scope)`
# æ„å»ºMarkdownæŠ¥å‘Šå†…å®¹


# %%
def build_report_content(analysis_results, resource_ids, scope):
    """ä¼˜åŒ–åçš„æŠ¥å‘Šç»“æ„"""
    # æ ¸å¿ƒæŒ‡æ ‡å¡ç‰‡å¼å¸ƒå±€
    content = f"""
# ğŸ“ {scope.capitalize()}ä½ç½®åˆ†ææŠ¥å‘Š  
**{analysis_results["time_range"][0]} è‡³ {analysis_results["time_range"][1]}**  

## ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡
| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |
|------|----|------|
| **æ€»è®°å½•** | {analysis_results["total_points"]} | ä½ç½®ç‚¹æ•°é‡ |
| **è¦†ç›–å¤©æ•°** | {analysis_results["unique_days"]} | æ•°æ®å®Œæ•´åº¦ |
| **æ´»åŠ¨åŠå¾„** | {analysis_results["distance_km"]:.2f}km | æœ€å¤§ç§»åŠ¨è·ç¦» |
| **æ—¶é—´æ–­å±‚** | {analysis_results["gap_stats"]["count"]} | æœ€é•¿é—´éš” {analysis_results["gap_stats"]["longest_gap"]:.1f}h |
"""

    # è®¾å¤‡ä½¿ç”¨é¥¼å›¾æ›¿ä»£è¡¨æ ¼
    device_chart = generate_device_pie_chart(analysis_results["device_stats"])
    content += f"""
## ğŸ“± è®¾å¤‡åˆ†å¸ƒ
![](:/{resource_ids["device_dist"]})
"""

    # ç²¾åº¦æŒ‡æ ‡å¡ç‰‡
    content += f"""
## ğŸ¯ å®šä½ç²¾åº¦
| æŒ‡æ ‡ | å€¼ |
|------|----|
| **æœ€ä½³ç²¾åº¦** | {analysis_results["accuracy_stats"]["min"]:.1f}m |
| **æœ€å·®ç²¾åº¦** | {analysis_results["accuracy_stats"]["max"]:.1f}m |
| **å¹³å‡ç²¾åº¦** | {analysis_results["accuracy_stats"]["mean"]:.1f}m |
"""

    # æ—¶é—´åˆ†å¸ƒçƒ­åŠ›å›¾
    content += f"""
## ğŸ•’ æ—¶é—´åˆ†å¸ƒ
![](:/{resource_ids["time_heatmap"]})
"""

    # ç²¾é€‰é‡è¦åœ°ç‚¹ï¼ˆå‰3ï¼‰
    content += """
## ğŸŒ å…³é”®åœ°ç‚¹
| ä½ç½® | è®¿é—® | åœç•™ | åæ ‡ |
|------|------|------|------|"""
    for i, place in enumerate(analysis_results["important_places"][:3]):
        visit_count = int(place["visit_count"])
        lat = place["latitude"]
        lon = place["longitude"]
        content += f"""
| **åœ°ç‚¹{i + 1}** | {visit_count}æ¬¡ | {place["avg_stay_min"]:.1f}åˆ† | [{lat}, {lon}]({generate_geo_link(lat, lon)}) |"""

    # å¯è§†åŒ–åˆ†æ
    content += f"""
## ğŸ“ˆ ç©ºé—´åˆ†æ
### ç§»åŠ¨è½¨è¿¹
![](:/{resource_ids["trajectory"]})

### ç²¾åº¦åˆ†å¸ƒ
![](:/{resource_ids["accuracy"]})
"""
    return content


# %% [markdown]
# ## æ›´æ–°Joplinç¬”è®°

# %% [markdown]
# ### `update_joplin_report(report_content, scope)`
# æ›´æ–°Joplinä½ç½®åˆ†ææŠ¥å‘Š


# %%
def update_joplin_report(report_content, scope):
    """
    æ›´æ–°Joplinä½ç½®åˆ†ææŠ¥å‘Š
    """
    note_title = f"ä½ç½®åˆ†ææŠ¥å‘Š_{scope}"
    existing_notes = searchnotes(f"title:{note_title}")

    if existing_notes:
        note_id = existing_notes[0].id
        # æ›´æ–°ç¬”è®°å†…å®¹
        updatenote_body(note_id, report_content)
    else:
        parent_id = searchnotebook("ewmobile")
        if not parent_id:
            parent_id = createnote(title="ewmobile", notebook=True)

        # åˆ›å»ºæ–°ç¬”è®°
        note_id = createnote(title=note_title, parent_id=parent_id, body=report_content)


# %% [markdown]
# ## ä¸»å‡½æ•°

# %% [markdown]
# ### `generate_location_reports()`
# ç”Ÿæˆä¸‰ä¸ªå±‚çº§çš„æŠ¥å‘Šï¼šæœˆæŠ¥ã€å­£æŠ¥ã€å¹´æŠ¥


# %%
def generate_location_reports():
    """
    ç”Ÿæˆä¸‰ä¸ªå±‚çº§çš„æŠ¥å‘Šï¼šæœˆæŠ¥ã€å­£æŠ¥ã€å¹´æŠ¥
    """
    for scope in REPORT_LEVELS.keys():
        log.info(f"å¼€å§‹ç”Ÿæˆ {scope} ä½ç½®æŠ¥å‘Š...")

        # 1. åŠ è½½æ•°æ®
        df = load_location_data(scope)
        if df.empty:
            log.warning(f"è·³è¿‡ {scope} æŠ¥å‘Šï¼Œæ— æ•°æ®")
            continue

        # åˆ†ææ•°æ®
        print(f"è¿›å…¥æ±‡æ€»è¾“å‡ºæ—¶æ•°æ®åˆ—å‘½ä»¤åˆ—è¡¨ä¸ºï¼š{df.columns.tolist()}")
        analysis_results = analyze_location_data(df, scope)

        # ç”Ÿæˆå¯è§†åŒ–å¹¶è·å–èµ„æºID
        resource_ids = generate_visualizations(df, analysis_results, scope)

        # æ„å»ºæŠ¥å‘Š
        report_content = build_report_content(analysis_results, resource_ids, scope)

        # æ›´æ–°ç¬”è®°å¹¶é™„åŠ èµ„æº
        update_joplin_report(report_content, scope)


# %% [markdown]
# ## ä¸»å…¥å£

# %% [markdown]
# ### `main()`
# è„šæœ¬ä¸»å…¥å£

# %%
if __name__ == "__main__":
    log.info("å¼€å§‹ç”Ÿæˆä½ç½®åˆ†ææŠ¥å‘Š...")
    generate_location_reports()
    log.info("ä½ç½®åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
